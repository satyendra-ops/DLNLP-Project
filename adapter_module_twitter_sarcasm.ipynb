{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "adapter_module_movie_reviews_ipynb_txt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kpe/bert-for-tf2/blob/master/examples/movie_reviews_with_bert_for_tf2_on_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFI2_B8ffipb"
      },
      "source": [
        "!pip install tqdm  >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Evlk1N78HIXM",
        "outputId": "846913c4-5fe7-4a9a-f72c-aa0d2e59d54d"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAdrQqEccIva"
      },
      "source": [
        "if tf.__version__.startswith(\"1.\"):\n",
        "  tf.enable_eager_execution()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA"
      },
      "source": [
        "!pip install bert-for-tf2 >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtI7cKWDbUVc"
      },
      "source": [
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "from bert.tokenization.bert_tokenization import FullTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skjp0GAdmOpA",
        "outputId": "be534cb2-0851-4ac9-c2ea-14351d777638"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy5WhtD7mb_U"
      },
      "source": [
        "import json\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "file='/content/drive/My Drive/twitter_sarcasm.jsonl'\r\n",
        "\r\n",
        "def parse_data(file):\r\n",
        "    for l in open(file,'r'):\r\n",
        "        yield json.loads(l)\r\n",
        "\r\n",
        "data = list(parse_data(file))\r\n",
        "sentences = []\r\n",
        "labels = []\r\n",
        "for item in data:\r\n",
        "    sentences.append(item['response'])\r\n",
        "    labels.append(item['label'])\r\n",
        "\r\n",
        "df=pd.DataFrame()\r\n",
        "df['sentence']=sentences\r\n",
        "df['polarity']=labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHntZKwvgpz1"
      },
      "source": [
        "df.sentence = df.sentence.replace('@USER', '', regex=True)\n",
        "df=df[df['polarity']=='SARCASM']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYSESwKXrtkY"
      },
      "source": [
        "data=pd.read_csv('/content/drive/My Drive/twitter_dataset.csv',encoding='latin-1',header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iA7xSD6s-Vl"
      },
      "source": [
        "negative=data[data[0]==0]\n",
        "positive=data[data[0]==4]\n",
        "data=pd.DataFrame()\n",
        "data=data.append(negative[:3000])\n",
        "data=data.append(positive[:3000])\n",
        "data=data.drop([1,2,3,4], axis=1)\n",
        "data=data.rename(columns={5: 'sentence',0:'polarity'})\n",
        "data=data[['sentence','polarity']]\n",
        "#data=data.append(df[:1000])\n",
        "data['polarity']=data['polarity'].replace({4: 1})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdaQHbeoXMOG",
        "outputId": "6a1004ca-afa8-4ecd-a523-8eec39d92ae8"
      },
      "source": [
        "import re\n",
        "for i in range(data['sentence'].shape[0]):\n",
        "  data['sentence'][i]=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",data['sentence'][i]).split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1zpvouFwWcj"
      },
      "source": [
        "data=data.sample(frac=1)\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "BkVGQ_XvZ4p_",
        "outputId": "bc16f00c-6ab9-4538-bc43-49f2c06268f3"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>question if you could get anything from austra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Algonquin agreed I saw the failwhale allllll d...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Just woke up</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Yeah Too bad people like a certain burrito eat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Flik You ll have to sweet talk her make sure s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>Bad day today just bad but loved seeing sancha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>My throat is still really sore I was meant to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>Happy Morning</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>Sad to say I just take multivitamins I guess I...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>AppleTV has died dreading diagnosis</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  polarity\n",
              "0     question if you could get anything from austra...         1\n",
              "1     Algonquin agreed I saw the failwhale allllll d...         0\n",
              "2                                          Just woke up         0\n",
              "3     Yeah Too bad people like a certain burrito eat...         0\n",
              "4     Flik You ll have to sweet talk her make sure s...         1\n",
              "...                                                 ...       ...\n",
              "5995  Bad day today just bad but loved seeing sancha...         1\n",
              "5996  My throat is still really sore I was meant to ...         0\n",
              "5997                                      Happy Morning         1\n",
              "5998  Sad to say I just take multivitamins I guess I...         1\n",
              "5999                AppleTV has died dreading diagnosis         0\n",
              "\n",
              "[6000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn"
      },
      "source": [
        "#Final Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abfwdn-g135"
      },
      "source": [
        "\n",
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "#from bert.tokenization import FullTokenizer\n",
        "from bert import bert_tokenization\n",
        "BertTokenizer = bert_tokenization.FullTokenizer\n",
        "\n",
        "class MovieReviewData:\n",
        "    DATA_COLUMN = \"sentence\"\n",
        "    LABEL_COLUMN = \"polarity\"\n",
        "\n",
        "    def __init__(self, tokenizer: FullTokenizer, df,sample_size, max_seq_len=1024):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sample_size = sample_size\n",
        "        print(sample_size)\n",
        "        self.max_seq_len = 0\n",
        "        '''train, test = download_and_load_datasets()\n",
        "        '''\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        train, test = train_test_split(df, test_size=0.33, random_state=42) \n",
        "        \n",
        "        train, test = map(lambda df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), \n",
        "                          [train, test])\n",
        "        \n",
        "\n",
        "        print(train)\n",
        "        if sample_size is not None:\n",
        "            assert sample_size % 128 == 0\n",
        "            train, test = train.head(sample_size), test.head(sample_size)\n",
        "            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
        "        \n",
        "        ((self.train_x, self.train_y),\n",
        "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
        "\n",
        "        print(\"max seq_len\", self.max_seq_len)\n",
        "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
        "        ((self.train_x, self.train_x_token_types),\n",
        "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
        "                                                       [self.train_x, self.test_x])\n",
        "\n",
        "    def _prepare(self, df):\n",
        "        x, y = [], []\n",
        "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
        "            for ndx, row in df.iterrows():\n",
        "                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n",
        "                tokens = self.tokenizer.tokenize(text)\n",
        "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
        "                x.append(token_ids)\n",
        "                y.append(int(label))\n",
        "                pbar.update()\n",
        "        return np.array(x), np.array(y)\n",
        "\n",
        "    def _pad(self, ids):\n",
        "        x, t = [], []\n",
        "        token_type_ids = [0] * self.max_seq_len\n",
        "        for input_ids in ids:\n",
        "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
        "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
        "            x.append(np.array(input_ids))\n",
        "            t.append(token_type_ids)\n",
        "        return np.array(x), np.array(t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGL0mEoNFGlP"
      },
      "source": [
        "## A tweak\n",
        "\n",
        "Because of a `tf.train.load_checkpoint` limitation requiring list permissions on the google storage bucket, we need to copy the pre-trained BERT weights locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_F488eixTV"
      },
      "source": [
        "bert_ckpt_dir=\"gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/\"\n",
        "bert_ckpt_file = bert_ckpt_dir + \"bert_model.ckpt\"\n",
        "bert_config_file = bert_ckpt_dir + \"bert_config.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGFfkWO07cWG",
        "outputId": "6f7ebf3b-d5c3-426a-a83f-393cf31a668f"
      },
      "source": [
        "\n",
        "%%time\n",
        "\n",
        "bert_model_dir=\"2018_10_18\"\n",
        "bert_model_name=\"uncased_L-12_H-768_A-12\"\n",
        "\n",
        "!mkdir -p .model .model/$bert_model_name\n",
        "\n",
        "for fname in [\"bert_config.json\", \"vocab.txt\", \"bert_model.ckpt.meta\", \"bert_model.ckpt.index\", \"bert_model.ckpt.data-00000-of-00001\"]:\n",
        "  cmd = f\"gsutil cp gs://bert_models/{bert_model_dir}/{bert_model_name}/{fname} .model/{bert_model_name}\"\n",
        "  !$cmd\n",
        "\n",
        "!ls -la .model .model/$bert_model_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_config.json...\n",
            "/ [1 files][  313.0 B/  313.0 B]                                                \n",
            "Operation completed over 1 objects/313.0 B.                                      \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/vocab.txt...\n",
            "/ [1 files][226.1 KiB/226.1 KiB]                                                \n",
            "Operation completed over 1 objects/226.1 KiB.                                    \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.meta...\n",
            "/ [1 files][883.0 KiB/883.0 KiB]                                                \n",
            "Operation completed over 1 objects/883.0 KiB.                                    \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.index...\n",
            "/ [1 files][  8.3 KiB/  8.3 KiB]                                                \n",
            "Operation completed over 1 objects/8.3 KiB.                                      \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001...\n",
            "\\ [1 files][420.0 MiB/420.0 MiB]                                                \n",
            "Operation completed over 1 objects/420.0 MiB.                                    \n",
            ".model:\n",
            "total 12\n",
            "drwxr-xr-x 3 root root 4096 Jan 20 12:49 .\n",
            "drwxr-xr-x 1 root root 4096 Jan 20 14:00 ..\n",
            "drwxr-xr-x 2 root root 4096 Jan 20 14:05 uncased_L-12_H-768_A-12\n",
            "\n",
            ".model/uncased_L-12_H-768_A-12:\n",
            "total 431244\n",
            "drwxr-xr-x 2 root root      4096 Jan 20 14:05 .\n",
            "drwxr-xr-x 3 root root      4096 Jan 20 12:49 ..\n",
            "-rw-r--r-- 1 root root       313 Jan 20 14:05 bert_config.json\n",
            "-rw-r--r-- 1 root root 440425712 Jan 20 14:05 bert_model.ckpt.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      8528 Jan 20 14:05 bert_model.ckpt.index\n",
            "-rw-r--r-- 1 root root    904243 Jan 20 14:05 bert_model.ckpt.meta\n",
            "-rw-r--r-- 1 root root    231508 Jan 20 14:05 vocab.txt\n",
            "CPU times: user 86.2 ms, sys: 246 ms, total: 332 ms\n",
            "Wall time: 19.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "049feT8dFprc"
      },
      "source": [
        "bert_ckpt_dir    = os.path.join(\".model/\",bert_model_name)\n",
        "bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
        "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4xPTleh2X2b"
      },
      "source": [
        "# Preparing the Data\n",
        "\n",
        "Now let's fetch and prepare the data by taking the first `max_seq_len` tokenens after tokenizing with the BERT tokenizer, und use `sample_size` examples for both training and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8WHJgzhIZf"
      },
      "source": [
        "To keep training fast, we'll take a sample of about 2500 train and test examples, respectively, and use the first 128 tokens only (transformers memory and computation requirements scale quadraticly with the sequence length - so with a TPU you might use `max_seq_len=512`, but on a GPU this would be too slow, and you will have to use a very small `batch_size`s to fit the model into the GPU memory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF_3KhGQ0GTc",
        "outputId": "3ed0b7a3-1f58-43fc-d25b-0b2e0a8002d9"
      },
      "source": [
        "%%time\n",
        "\n",
        "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
        "data = MovieReviewData(tokenizer,data, \n",
        "                       sample_size=None,#5000, \n",
        "                       max_seq_len=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 11%|█▏        | 456/4.02k [00:00<00:00, 4.55kit/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "                                               sentence  polarity\n",
            "3635                                                            0\n",
            "4688                                                            0\n",
            "539                                                             0\n",
            "4680                                                            1\n",
            "816                                                             1\n",
            "...                                                 ...       ...\n",
            "1050  flat out today didnt get everythin done amp en...         0\n",
            "3171  Whinging My client amp boss don t understand E...         0\n",
            "3565  Was a quot ranger quot for a day sporting red ...         1\n",
            "1236  Used the term quot Fail Whale quot to a client...         0\n",
            "5834  tks pa 4 quot tapauing quot croissant tuna kno...         0\n",
            "\n",
            "[4020 rows x 2 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.02k/4.02k [00:01<00:00, 2.52kit/s]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "100%|██████████| 1.98k/1.98k [00:00<00:00, 2.46kit/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max seq_len 49\n",
            "CPU times: user 2.6 s, sys: 0 ns, total: 2.6 s\n",
            "Wall time: 2.61 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prRQM8pDi8xI",
        "outputId": "a39344ef-1189-4cfc-8e82-287ae72e61b8"
      },
      "source": [
        "print(\"            train_x\", data.train_x.shape)\n",
        "print(\"train_x_token_types\", data.train_x_token_types.shape)\n",
        "print(\"            train_y\", data.train_y.shape)\n",
        "\n",
        "print(\"             test_x\", data.test_x.shape)\n",
        "\n",
        "print(\"        max_seq_len\", data.max_seq_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            train_x (4020, 49)\n",
            "train_x_token_types (4020, 49)\n",
            "            train_y (4020,)\n",
            "             test_x (1980, 49)\n",
            "        max_seq_len 49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRnHSz3iSXz"
      },
      "source": [
        "## Adapter BERT\n",
        "\n",
        "If we decide to use [adapter-BERT](https://arxiv.org/abs/1902.00751) we need some helpers for freezing the original BERT layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it"
      },
      "source": [
        "\n",
        "def flatten_layers(root_layer):\n",
        "    if isinstance(root_layer, keras.layers.Layer):\n",
        "        yield root_layer\n",
        "    for layer in root_layer._layers:\n",
        "        for sub_layer in flatten_layers(layer):\n",
        "            yield sub_layer\n",
        "\n",
        "\n",
        "def freeze_bert_layers(l_bert):\n",
        "    \"\"\"\n",
        "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
        "    \"\"\"\n",
        "    for layer in flatten_layers(l_bert):\n",
        "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
        "            layer.trainable = True\n",
        "        elif len(layer._layers) == 0:\n",
        "            layer.trainable = False\n",
        "        l_bert.embeddings_layer.trainable = False\n",
        "\n",
        "\n",
        "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
        "                                   end_learn_rate=1e-7,\n",
        "                                   warmup_epoch_count=10,\n",
        "                                   total_epoch_count=90):\n",
        "\n",
        "    def lr_scheduler(epoch):\n",
        "        if epoch < warmup_epoch_count:\n",
        "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
        "        else:\n",
        "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
        "        return float(res)\n",
        "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "\n",
        "    return learning_rate_scheduler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now let's create a classification model using [adapter-BERT](https//arxiv.org/abs/1902.00751), which is clever way of reducing the trainable parameter count, by freezing the original BERT weights, and adapting them with two FFN bottlenecks (i.e. `adapter_size` bellow) in every BERT layer.\n",
        "\n",
        "**N.B.** The commented out code below show how to feed a `token_type_ids`/`segment_ids` sequence (which is not needed in our case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq"
      },
      "source": [
        "def create_model(max_seq_len, adapter_size=64):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  #adapter_size = 64  # see - arXiv:1902.00751\n",
        "\n",
        "  # create the bert layer\n",
        "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
        "      bc = StockBertConfig.from_json_string(reader.read())\n",
        "      bert_params = map_stock_config_to_params(bc)\n",
        "      bert_params.adapter_size = adapter_size\n",
        "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
        "        \n",
        "  input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
        "  # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
        "  # output         = bert([input_ids, token_type_ids])\n",
        "  output         = bert(input_ids)\n",
        "\n",
        "  print(\"bert shape\", output.shape)\n",
        "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
        "  cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
        "  logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
        "  logits = keras.layers.Dropout(0.5)(logits)\n",
        "  logits = keras.layers.Dense(units=2, activation=\"softmax\")(logits)\n",
        "\n",
        "  # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
        "  # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
        "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
        "  model.build(input_shape=(None, max_seq_len))\n",
        "\n",
        "  # load the pre-trained model weights\n",
        "  load_stock_weights(bert, bert_ckpt_file)\n",
        "\n",
        "  # freeze weights if adapter-BERT is used\n",
        "  if adapter_size is not None:\n",
        "      freeze_bert_layers(bert)\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
        "\n",
        "  model.summary()\n",
        "        \n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZnmtDc7HlEm",
        "outputId": "e315ab4d-e477-452b-a656-70084f1e8960"
      },
      "source": [
        "import keras\n",
        "adapter_size = 64 # use None to fine-tune all of BERT\n",
        "model = create_model(data.max_seq_len, adapter_size=adapter_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert shape (None, 49, 768)\n",
            "loader: No value for:[bert/encoder/layer_0/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_0/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_0/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_0/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_0/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_0/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_0/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_0/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_0/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_0/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_0/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_0/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_1/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_1/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_1/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_1/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_1/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_1/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_1/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_1/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_1/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_1/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_1/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_1/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_2/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_2/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_2/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_2/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_2/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_2/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_2/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_2/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_2/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_2/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_2/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_2/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_3/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_3/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_3/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_3/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_3/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_3/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_3/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_3/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_3/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_3/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_3/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_3/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_4/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_4/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_4/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_4/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_4/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_4/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_4/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_4/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_4/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_4/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_4/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_4/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_5/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_5/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_5/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_5/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_5/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_5/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_5/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_5/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_5/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_5/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_5/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_5/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_6/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_6/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_6/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_6/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_6/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_6/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_6/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_6/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_6/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_6/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_6/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_6/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_7/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_7/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_7/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_7/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_7/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_7/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_7/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_7/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_7/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_7/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_7/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_7/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_8/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_8/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_8/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_8/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_8/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_8/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_8/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_8/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_8/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_8/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_8/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_8/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_9/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_9/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_9/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_9/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_9/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_9/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_9/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_9/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_9/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_9/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_9/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_9/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_10/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_10/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_10/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_10/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_10/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_10/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_10/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_10/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_10/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_10/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_10/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_10/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_11/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_11/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_11/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_11/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_11/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_11/output/adapter-down/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_11/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_11/output/adapter-down/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_11/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_11/output/adapter-up/kernel] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "loader: No value for:[bert/encoder/layer_11/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_11/output/adapter-up/bias] in:[.model/uncased_L-12_H-768_A-12/bert_model.ckpt]\n",
            "Done loading 196 BERT weights from: .model/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7fcfc32c07b8> (prefix:bert). Count of weights not found in the checkpoint was: [96]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 49)]              0         \n",
            "_________________________________________________________________\n",
            "bert (BertModelLayer)        (None, 49, 768)           111269376 \n",
            "_________________________________________________________________\n",
            "lambda_4 (Lambda)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2)                 1538      \n",
            "=================================================================\n",
            "Total params: 111,861,506\n",
            "Trainable params: 3,008,258\n",
            "Non-trainable params: 108,853,248\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuLOkwonF-9S",
        "outputId": "c7676cf7-0a1c-4041-d69f-d21bb3bf3852"
      },
      "source": [
        "%%time\n",
        "\n",
        "log_dir = \".log/movie_reviews/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "total_epoch_count = 30\n",
        "# model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,\n",
        "model.fit(x=data.train_x, y=data.train_y,\n",
        "          validation_split=0.1,\n",
        "          batch_size=32,\n",
        "          shuffle=True,\n",
        "          epochs=total_epoch_count,\n",
        "          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
        "                                                    end_learn_rate=1e-7,\n",
        "                                                    warmup_epoch_count=20,\n",
        "                                                    total_epoch_count=total_epoch_count),\n",
        "                     keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
        "                     tensorboard_callback])\n",
        "\n",
        "model.save_weights('./twitter_sarcasm.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 5.000000000000001e-07.\n",
            "114/114 [==============================] - 117s 796ms/step - loss: 0.8918 - acc: 0.4916 - val_loss: 0.6877 - val_acc: 0.5871\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 1.0000000000000002e-06.\n",
            "114/114 [==============================] - 82s 721ms/step - loss: 0.8769 - acc: 0.5042 - val_loss: 0.6866 - val_acc: 0.5846\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 1.5000000000000002e-06.\n",
            "114/114 [==============================] - 82s 720ms/step - loss: 0.8732 - acc: 0.5061 - val_loss: 0.6857 - val_acc: 0.5697\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 2.0000000000000003e-06.\n",
            "114/114 [==============================] - 82s 718ms/step - loss: 0.8675 - acc: 0.5060 - val_loss: 0.6837 - val_acc: 0.5721\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 2.5000000000000006e-06.\n",
            "114/114 [==============================] - 82s 716ms/step - loss: 0.8651 - acc: 0.5077 - val_loss: 0.6828 - val_acc: 0.5647\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 3.0000000000000005e-06.\n",
            "114/114 [==============================] - 82s 720ms/step - loss: 0.8569 - acc: 0.4800 - val_loss: 0.6795 - val_acc: 0.5672\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 3.5000000000000004e-06.\n",
            "114/114 [==============================] - 82s 722ms/step - loss: 0.8536 - acc: 0.5016 - val_loss: 0.6797 - val_acc: 0.5672\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 4.000000000000001e-06.\n",
            "114/114 [==============================] - 82s 722ms/step - loss: 0.8062 - acc: 0.5354 - val_loss: 0.6791 - val_acc: 0.5522\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 4.500000000000001e-06.\n",
            "114/114 [==============================] - 82s 722ms/step - loss: 0.8123 - acc: 0.5192 - val_loss: 0.6714 - val_acc: 0.5846\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 5.000000000000001e-06.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.7929 - acc: 0.5193 - val_loss: 0.6620 - val_acc: 0.6169\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 5.500000000000001e-06.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.7928 - acc: 0.4863 - val_loss: 0.6583 - val_acc: 0.6219\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 6.000000000000001e-06.\n",
            "114/114 [==============================] - 82s 722ms/step - loss: 0.7692 - acc: 0.5282 - val_loss: 0.6489 - val_acc: 0.6368\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 6.500000000000001e-06.\n",
            "114/114 [==============================] - 82s 720ms/step - loss: 0.7542 - acc: 0.5341 - val_loss: 0.6451 - val_acc: 0.6642\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 7.000000000000001e-06.\n",
            "114/114 [==============================] - 82s 720ms/step - loss: 0.7452 - acc: 0.5358 - val_loss: 0.6348 - val_acc: 0.6791\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 7.500000000000001e-06.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.7281 - acc: 0.5556 - val_loss: 0.6204 - val_acc: 0.6567\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 8.000000000000001e-06.\n",
            "114/114 [==============================] - 82s 722ms/step - loss: 0.7147 - acc: 0.5647 - val_loss: 0.6126 - val_acc: 0.6866\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 8.500000000000002e-06.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.6685 - acc: 0.6379 - val_loss: 0.6034 - val_acc: 0.6891\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 9.000000000000002e-06.\n",
            "114/114 [==============================] - 82s 720ms/step - loss: 0.6473 - acc: 0.6499 - val_loss: 0.6029 - val_acc: 0.6990\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 9.500000000000002e-06.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.5957 - acc: 0.6857 - val_loss: 0.6074 - val_acc: 0.7139\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 1.0000000000000003e-05.\n",
            "114/114 [==============================] - 82s 722ms/step - loss: 0.5621 - acc: 0.7241 - val_loss: 0.6065 - val_acc: 0.7438\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 6.57933224657568e-06.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.5340 - acc: 0.7356 - val_loss: 0.6049 - val_acc: 0.7363\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 4.328761281083059e-06.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.5540 - acc: 0.7200 - val_loss: 0.6068 - val_acc: 0.7438\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 2.8480358684358016e-06.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.5185 - acc: 0.7553 - val_loss: 0.6079 - val_acc: 0.7413\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 1.873817422860384e-06.\n",
            "114/114 [==============================] - 82s 721ms/step - loss: 0.5140 - acc: 0.7499 - val_loss: 0.6062 - val_acc: 0.7413\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 1.2328467394420658e-06.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.5410 - acc: 0.7437 - val_loss: 0.6047 - val_acc: 0.7388\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 8.11130830789687e-07.\n",
            "114/114 [==============================] - 82s 720ms/step - loss: 0.5175 - acc: 0.7554 - val_loss: 0.6056 - val_acc: 0.7413\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 5.336699231206311e-07.\n",
            "114/114 [==============================] - 82s 721ms/step - loss: 0.5401 - acc: 0.7325 - val_loss: 0.6046 - val_acc: 0.7388\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 3.511191734215131e-07.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.5261 - acc: 0.7497 - val_loss: 0.6052 - val_acc: 0.7438\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 2.310129700083159e-07.\n",
            "114/114 [==============================] - 82s 719ms/step - loss: 0.5274 - acc: 0.7351 - val_loss: 0.6054 - val_acc: 0.7438\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 1.5199110829529328e-07.\n",
            "114/114 [==============================] - 82s 720ms/step - loss: 0.5214 - acc: 0.7472 - val_loss: 0.6051 - val_acc: 0.7388\n",
            "CPU times: user 22min 53s, sys: 9min 30s, total: 32min 23s\n",
            "Wall time: 41min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSqMu64oHzqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e9d99a-fcb5-4f8e-8167-3557128f56e1"
      },
      "source": [
        "%%time\n",
        "\n",
        "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
        "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
        "\n",
        "print(\"train acc\", train_acc)\n",
        "print(\" test acc\", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "126/126 [==============================] - 41s 327ms/step - loss: 0.4734 - acc: 0.7803\n",
            "62/62 [==============================] - 20s 328ms/step - loss: 0.5157 - acc: 0.7601\n",
            "train acc 0.7803482413291931\n",
            " test acc 0.7601010203361511\n",
            "CPU times: user 27.6 s, sys: 5.19 s, total: 32.8 s\n",
            "Wall time: 1min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSKDZEnVabnl"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "To evaluate the trained model, let's load the saved weights in a new model instance, and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "qCpabQ15WS3U",
        "outputId": "220b2b55-dd78-4795-d3b6-d364c8323734"
      },
      "source": [
        "%%time \n",
        "\n",
        "model = create_model(data.max_seq_len, adapter_size=None)\n",
        "model.load_weights(\"movie_reviews.h5\")\n",
        "\n",
        "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
        "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
        "\n",
        "print(\"train acc\", train_acc)\n",
        "print(\" test acc\", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert shape (?, 128, 768)\n",
            "Done loading 196 BERT weights from: .model/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f671451f780> (prefix:bert_1)\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 1538      \n",
            "=================================================================\n",
            "Total params: 109,482,242\n",
            "Trainable params: 109,482,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "2560/2560 [==============================] - 30s 12ms/sample - loss: 0.3414 - acc: 0.9723\n",
            "2560/2560 [==============================] - 28s 11ms/sample - loss: 0.3904 - acc: 0.9207\n",
            "train acc 0.9722656\n",
            " test acc 0.9207031\n",
            "CPU times: user 45 s, sys: 23 s, total: 1min 7s\n",
            "Wall time: 1min 9s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uzdOFQ5awM1"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "For prediction, we need to prepare the input text the same way as we did for training - tokenize, adding the special `[CLS]` and `[SEP]` token at begin and end of the token sequence, and pad to match the model input shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "m7dAAoCuW1xj",
        "outputId": "15f4a565-1659-4c7a-a21a-5a1322389746"
      },
      "source": [
        "pred_sentences = [\n",
        "  \"That movie was absolutely awful\",\n",
        "  \"The acting was a bit lacking\",\n",
        "  \"The film was creative and surprising\",\n",
        "  \"Absolutely fantastic!\"\n",
        "]\n",
        "\n",
        "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
        "pred_tokens    = map(tokenizer.tokenize, pred_sentences)\n",
        "pred_tokens    = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
        "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
        "\n",
        "pred_token_ids = map(lambda tids: tids +[0]*(data.max_seq_len-len(tids)),pred_token_ids)\n",
        "pred_token_ids = np.array(list(pred_token_ids))\n",
        "\n",
        "print('pred_token_ids', pred_token_ids.shape)\n",
        "\n",
        "res = model.predict(pred_token_ids).argmax(axis=-1)\n",
        "\n",
        "for text, sentiment in zip(pred_sentences, res):\n",
        "  print(\" text:\", text)\n",
        "  print(\"  res:\", [\"negative\",\"positive\"][sentiment])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred_token_ids (4, 128)\n",
            " text: That movie was absolutely awful\n",
            "  res: negative\n",
            " text: The acting was a bit lacking\n",
            "  res: negative\n",
            " text: The film was creative and surprising\n",
            "  res: positive\n",
            " text: Absolutely fantastic!\n",
            "  res: positive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}